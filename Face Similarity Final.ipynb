{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cdcf356",
   "metadata": {},
   "source": [
    "### Face Recognition Using Deep Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96608657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\AniAarya\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Confirm that 1.jpg exists",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m img1_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m img2_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 9\u001b[0m result \u001b[38;5;241m=\u001b[39m DeepFace\u001b[38;5;241m.\u001b[39mverify(img1_path, img2_path)\n\u001b[0;32m     11\u001b[0m img1 \u001b[38;5;241m=\u001b[39m mpimg\u001b[38;5;241m.\u001b[39mimread(img1_path)\n\u001b[0;32m     12\u001b[0m img2 \u001b[38;5;241m=\u001b[39m mpimg\u001b[38;5;241m.\u001b[39mimread(img2_path)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\deepface\\DeepFace.py:158\u001b[0m, in \u001b[0;36mverify\u001b[1;34m(img1_path, img2_path, model_name, detector_backend, distance_metric, enforce_detection, align, normalization)\u001b[0m\n\u001b[0;32m    155\u001b[0m target_size \u001b[38;5;241m=\u001b[39m functions\u001b[38;5;241m.\u001b[39mfind_target_size(model_name\u001b[38;5;241m=\u001b[39mmodel_name)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;66;03m# img pairs might have many faces\u001b[39;00m\n\u001b[1;32m--> 158\u001b[0m img1_objs \u001b[38;5;241m=\u001b[39m functions\u001b[38;5;241m.\u001b[39mextract_faces(\n\u001b[0;32m    159\u001b[0m     img\u001b[38;5;241m=\u001b[39mimg1_path,\n\u001b[0;32m    160\u001b[0m     target_size\u001b[38;5;241m=\u001b[39mtarget_size,\n\u001b[0;32m    161\u001b[0m     detector_backend\u001b[38;5;241m=\u001b[39mdetector_backend,\n\u001b[0;32m    162\u001b[0m     grayscale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    163\u001b[0m     enforce_detection\u001b[38;5;241m=\u001b[39menforce_detection,\n\u001b[0;32m    164\u001b[0m     align\u001b[38;5;241m=\u001b[39malign,\n\u001b[0;32m    165\u001b[0m )\n\u001b[0;32m    167\u001b[0m img2_objs \u001b[38;5;241m=\u001b[39m functions\u001b[38;5;241m.\u001b[39mextract_faces(\n\u001b[0;32m    168\u001b[0m     img\u001b[38;5;241m=\u001b[39mimg2_path,\n\u001b[0;32m    169\u001b[0m     target_size\u001b[38;5;241m=\u001b[39mtarget_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    173\u001b[0m     align\u001b[38;5;241m=\u001b[39malign,\n\u001b[0;32m    174\u001b[0m )\n\u001b[0;32m    175\u001b[0m \u001b[38;5;66;03m# --------------------------------\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\deepface\\commons\\functions.py:165\u001b[0m, in \u001b[0;36mextract_faces\u001b[1;34m(img, target_size, detector_backend, grayscale, enforce_detection, align)\u001b[0m\n\u001b[0;32m    162\u001b[0m extracted_faces \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# img might be path, base64 or numpy array. Convert it to numpy whatever it is.\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m img, img_name \u001b[38;5;241m=\u001b[39m load_image(img)\n\u001b[0;32m    166\u001b[0m img_region \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m detector_backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\deepface\\commons\\functions.py:118\u001b[0m, in \u001b[0;36mload_image\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# The image is a path\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(img) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfirm that \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m exists\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# image must be a file on the system then\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# image name must have english characters\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m img\u001b[38;5;241m.\u001b[39misascii() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Confirm that 1.jpg exists"
     ]
    }
   ],
   "source": [
    "from deepface import DeepFace\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "similarity_threshold = 100\n",
    "img1_path = \"20.jpg\"\n",
    "img2_path = \"21.jpg\"\n",
    "\n",
    "result = DeepFace.verify(img1_path, img2_path)\n",
    "\n",
    "img1 = mpimg.imread(img1_path)\n",
    "img2 = mpimg.imread(img2_path)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax1.imshow(img1)\n",
    "ax1.set_title('Image 1')\n",
    "\n",
    "ax2.imshow(img2)\n",
    "ax2.set_title('Image 2')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "similarity_percentage = round(result[\"distance\"] * 100, 2)\n",
    "confidence_score=similarity_threshold - similarity_percentage\n",
    "if result[\"verified\"]:\n",
    "    print(f\"Both faces are similar.\\n Confidence Score is {confidence_score}\")\n",
    "else:\n",
    "    print(f\"Both faces are dissimilar.\\n Confidence Score is {confidence_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b397854",
   "metadata": {},
   "source": [
    "### Face Similarity Using Face Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2c2241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1, prewhiten\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Load FaceNet model\n",
    "model = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "\n",
    "# Create MTCNN face detector\n",
    "mtcnn = MTCNN(keep_all=True, min_face_size=20)\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    img_cropped = mtcnn(img)\n",
    "    \n",
    "    if img_cropped is not None:\n",
    "        img_cropped = transforms.functional.resize(img_cropped, (160, 160))\n",
    "        img_cropped = prewhiten(img_cropped)\n",
    "\n",
    "        if not isinstance(img_cropped, torch.Tensor):\n",
    "            img_cropped_tensor = transforms.functional.to_tensor(img_cropped)\n",
    "        else:\n",
    "            img_cropped_tensor = img_cropped.clone()\n",
    "        \n",
    "        return img_cropped_tensor, img\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "def calculate_similarity(img1, img2, model):\n",
    "    with torch.no_grad():\n",
    "        img1_embedding = model(img1)\n",
    "        img2_embedding = model(img2)\n",
    "\n",
    "    distance = F.pairwise_distance(img1_embedding, img2_embedding)\n",
    "    similarity_percentage = 100 - distance.item()\n",
    "\n",
    "    return similarity_percentage, img1_embedding.cpu().detach(), img2_embedding.cpu().detach()\n",
    "\n",
    "# File paths\n",
    "img1_path = \"2.jpg\"\n",
    "img2_path = \"3.jpg\"\n",
    "\n",
    "# Preprocess images\n",
    "img1_tensor, img1_pil = preprocess_image(img1_path)\n",
    "img2_tensor, img2_pil = preprocess_image(img2_path)\n",
    "\n",
    "if img1_tensor is not None and img2_tensor is not None:\n",
    "    # Calculate similarity\n",
    "    confidence_percentage, _, _ = calculate_similarity(img1_tensor, img2_tensor, model)\n",
    "    similarity_threshold = 99.0\n",
    "\n",
    "    # Visualize images\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    ax1.imshow(img1_pil)\n",
    "    ax1.set_title('Image 1')\n",
    "\n",
    "    ax2.imshow(img2_pil)\n",
    "    ax2.set_title('Image 2')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Display result message\n",
    "    if confidence_percentage >= similarity_threshold:\n",
    "        print(f\"Images are Similar.\\nConfidence score: {confidence_percentage:.2f}%\")\n",
    "    else:\n",
    "        print(f\"Imaeges are Dissimilar.\\nConfidence score: {confidence_percentage:.2f}%\")\n",
    "else:\n",
    "    print(\"No face detected in one or both of the images.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d40b351",
   "metadata": {},
   "source": [
    "### Face Similarity using VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79581130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg19 import VGG19, preprocess_input\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_input(img_array)\n",
    "    return img_array\n",
    "\n",
    "def calculate_similarity(img1_path, img2_path, model):\n",
    "    img1_array = preprocess_image(img1_path)\n",
    "    img2_array = preprocess_image(img2_path)\n",
    "\n",
    "    img1_embedding = model.predict(img1_array)\n",
    "    img2_embedding = model.predict(img2_array)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarity_score = np.dot(img1_embedding.flatten(), img2_embedding.flatten()) / (np.linalg.norm(img1_embedding) * np.linalg.norm(img2_embedding))\n",
    "    similarity_percentage = similarity_score * 100\n",
    "\n",
    "    return similarity_percentage\n",
    "\n",
    "# File paths\n",
    "img1_path = \"Ranveer2.jpg\"\n",
    "img2_path = \"ranveer.jpg\"\n",
    "\n",
    "# Load VGG19 model\n",
    "vgg_model = VGG19(weights='imagenet', include_top=False)\n",
    "\n",
    "# Calculate similarity\n",
    "similarity_percentage = calculate_similarity(img1_path, img2_path, vgg_model)\n",
    "\n",
    "# Visualize images\n",
    "img1 = mpimg.imread(img1_path)\n",
    "img2 = mpimg.imread(img2_path)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax1.imshow(img1)\n",
    "ax1.set_title('Image 1')\n",
    "\n",
    "ax2.imshow(img2)\n",
    "ax2.set_title('Image 2')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Display result message\n",
    "print(\"Are these faces the same person?\", similarity_percentage > 50)\n",
    "print(f\"Similarity score: {similarity_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4415dd",
   "metadata": {},
   "source": [
    "### Face Similarity Using VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f4e8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_input(img_array)\n",
    "    return img_array\n",
    "\n",
    "def calculate_similarity(img1_path, img2_path, model):\n",
    "    img1_array = preprocess_image(img1_path)\n",
    "    img2_array = preprocess_image(img2_path)\n",
    "\n",
    "    # Get VGG16 embeddings\n",
    "    img1_embedding = model.predict(img1_array)\n",
    "    img2_embedding = model.predict(img2_array)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarity_score = np.dot(img1_embedding.flatten(), img2_embedding.flatten()) / (np.linalg.norm(img1_embedding) * np.linalg.norm(img2_embedding))\n",
    "    similarity_percentage = similarity_score * 100\n",
    "\n",
    "    return similarity_percentage\n",
    "\n",
    "# File paths\n",
    "img1_path = \"Ranveer2.jpg\"\n",
    "img2_path = \"ranveer.jpg\"\n",
    "\n",
    "# Load VGG16 model\n",
    "vgg_model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "# Calculate similarity\n",
    "similarity_percentage = calculate_similarity(img1_path, img2_path, vgg_model)\n",
    "\n",
    "# Visualize images\n",
    "img1 = mpimg.imread(img1_path)\n",
    "img2 = mpimg.imread(img2_path)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax1.imshow(img1)\n",
    "ax1.set_title('Image 1')\n",
    "\n",
    "ax2.imshow(img2)\n",
    "ax2.set_title('Image 2')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Display result message\n",
    "print(\"Are these faces the same person?\", similarity_percentage > 50)\n",
    "print(f\"Similarity score: {similarity_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c9cf9d",
   "metadata": {},
   "source": [
    "### Face Similarity Model Using Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7524ba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "similarity_threshold = 100\n",
    "\n",
    "# Load pre-trained ResNet model\n",
    "resnet_model = ResNet50(weights='imagenet', include_top=False)\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_input(img_array)\n",
    "    return img_array\n",
    "\n",
    "def calculate_similarity(img1_path, img2_path, model):\n",
    "    img1_array = preprocess_image(img1_path)\n",
    "    img2_array = preprocess_image(img2_path)\n",
    "\n",
    "    img1_embedding = model.predict(img1_array)\n",
    "    img2_embedding = model.predict(img2_array)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarity_score = np.dot(img1_embedding.flatten(), img2_embedding.flatten()) / (np.linalg.norm(img1_embedding) * np.linalg.norm(img2_embedding))\n",
    "    similarity_percentage = similarity_score * 100\n",
    "\n",
    "    return similarity_percentage\n",
    "\n",
    "# File paths\n",
    "img1_path = \"3.jpg\"\n",
    "img2_path = \"4.jpg\"\n",
    "\n",
    "# Calculate similarity\n",
    "similarity_percentage = calculate_similarity(img1_path, img2_path, resnet_model)\n",
    "\n",
    "# Visualize images\n",
    "img1 = mpimg.imread(img1_path)\n",
    "img2 = mpimg.imread(img2_path)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax1.imshow(img1)\n",
    "ax1.set_title('Image 1')\n",
    "\n",
    "ax2.imshow(img2)\n",
    "ax2.set_title('Image 2')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Display result message\n",
    "confidence_score = similarity_threshold - similarity_percentage\n",
    "if similarity_percentage > similarity_threshold:\n",
    "    print(f\"Both faces are similar.\\nScore is {confidence_score}\")\n",
    "else:\n",
    "    print(f\"Both faces are dissimilar.\\nScore is {confidence_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e3978b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
